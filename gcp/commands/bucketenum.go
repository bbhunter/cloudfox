package commands

import (
	"context"
	"fmt"
	"strings"
	"sync"

	bucketenumservice "github.com/BishopFox/cloudfox/gcp/services/bucketEnumService"
	"github.com/BishopFox/cloudfox/globals"
	"github.com/BishopFox/cloudfox/internal"
	gcpinternal "github.com/BishopFox/cloudfox/internal/gcp"
	"github.com/spf13/cobra"
)

var (
	bucketEnumMaxObjects int
)

var GCPBucketEnumCommand = &cobra.Command{
	Use:     globals.GCP_BUCKETENUM_MODULE_NAME,
	Aliases: []string{"bucket-scan", "gcs-enum", "sensitive-files"},
	Short:   "Enumerate GCS buckets for sensitive files (credentials, secrets, configs)",
	Long: `Enumerate GCS buckets to find potentially sensitive files.

This module scans bucket contents for files that may contain:
- Credentials (service account keys, SSH keys, certificates)
- Secrets (environment files, API keys, tokens)
- Configuration files (may contain hardcoded secrets)
- Database backups
- Terraform state files
- Source code/git repositories

File categories detected:
- Credential: .json keys, .pem, .key, .p12, SSH keys
- Secret: .env, passwords, API keys, tokens
- Config: YAML, properties, settings files
- Backup: SQL dumps, archives
- Source: Git repositories
- Cloud: Cloud Functions source, build artifacts

WARNING: This may take a long time for buckets with many objects.
Use --max-objects to limit the scan.`,
	Run: runGCPBucketEnumCommand,
}

func init() {
	GCPBucketEnumCommand.Flags().IntVar(&bucketEnumMaxObjects, "max-objects", 1000, "Maximum objects to scan per bucket (0 for unlimited)")
}

type BucketEnumModule struct {
	gcpinternal.BaseGCPModule
	SensitiveFiles []bucketenumservice.SensitiveFileInfo
	LootMap        map[string]*internal.LootFile
	mu             sync.Mutex
}

type BucketEnumOutput struct {
	Table []internal.TableFile
	Loot  []internal.LootFile
}

func (o BucketEnumOutput) TableFiles() []internal.TableFile { return o.Table }
func (o BucketEnumOutput) LootFiles() []internal.LootFile   { return o.Loot }

func runGCPBucketEnumCommand(cmd *cobra.Command, args []string) {
	cmdCtx, err := gcpinternal.InitializeCommandContext(cmd, globals.GCP_BUCKETENUM_MODULE_NAME)
	if err != nil {
		return
	}

	module := &BucketEnumModule{
		BaseGCPModule:  gcpinternal.NewBaseGCPModule(cmdCtx),
		SensitiveFiles: []bucketenumservice.SensitiveFileInfo{},
		LootMap:        make(map[string]*internal.LootFile),
	}
	module.initializeLootFiles()
	module.Execute(cmdCtx.Ctx, cmdCtx.Logger)
}

func (m *BucketEnumModule) Execute(ctx context.Context, logger internal.Logger) {
	logger.InfoM(fmt.Sprintf("Scanning buckets for sensitive files (max %d objects per bucket)...", bucketEnumMaxObjects), globals.GCP_BUCKETENUM_MODULE_NAME)
	m.RunProjectEnumeration(ctx, logger, m.ProjectIDs, globals.GCP_BUCKETENUM_MODULE_NAME, m.processProject)

	if len(m.SensitiveFiles) == 0 {
		logger.InfoM("No sensitive files found", globals.GCP_BUCKETENUM_MODULE_NAME)
		return
	}

	// Count by risk level
	criticalCount := 0
	highCount := 0
	for _, file := range m.SensitiveFiles {
		switch file.RiskLevel {
		case "CRITICAL":
			criticalCount++
		case "HIGH":
			highCount++
		}
	}

	logger.SuccessM(fmt.Sprintf("Found %d potentially sensitive file(s) (%d CRITICAL, %d HIGH)",
		len(m.SensitiveFiles), criticalCount, highCount), globals.GCP_BUCKETENUM_MODULE_NAME)
	m.writeOutput(ctx, logger)
}

func (m *BucketEnumModule) processProject(ctx context.Context, projectID string, logger internal.Logger) {
	if globals.GCP_VERBOSITY >= globals.GCP_VERBOSE_ERRORS {
		logger.InfoM(fmt.Sprintf("Scanning buckets in project: %s", projectID), globals.GCP_BUCKETENUM_MODULE_NAME)
	}

	svc := bucketenumservice.New()

	// Get list of buckets
	buckets, err := svc.GetBucketsList(projectID)
	if err != nil {
		m.CommandCounter.Error++
		gcpinternal.HandleGCPError(err, logger, globals.GCP_BUCKETENUM_MODULE_NAME,
			fmt.Sprintf("Could not enumerate buckets in project %s", projectID))
		return
	}

	if globals.GCP_VERBOSITY >= globals.GCP_VERBOSE_ERRORS {
		logger.InfoM(fmt.Sprintf("Found %d bucket(s) in project %s", len(buckets), projectID), globals.GCP_BUCKETENUM_MODULE_NAME)
	}

	// Scan each bucket
	for _, bucketName := range buckets {
		files, err := svc.EnumerateBucketSensitiveFiles(bucketName, projectID, bucketEnumMaxObjects)
		if err != nil {
			m.CommandCounter.Error++
			gcpinternal.HandleGCPError(err, logger, globals.GCP_BUCKETENUM_MODULE_NAME,
				fmt.Sprintf("Could not scan bucket %s in project %s", bucketName, projectID))
			continue
		}

		m.mu.Lock()
		m.SensitiveFiles = append(m.SensitiveFiles, files...)
		for _, file := range files {
			m.addFileToLoot(file)
		}
		m.mu.Unlock()
	}
}

func (m *BucketEnumModule) initializeLootFiles() {
	m.LootMap["bucket-enum-sensitive-commands"] = &internal.LootFile{
		Name:     "bucket-enum-sensitive-commands",
		Contents: "# GCS Download Commands for CRITICAL/HIGH Risk Files\n# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n",
	}
	m.LootMap["bucket-enum-commands"] = &internal.LootFile{
		Name:     "bucket-enum-commands",
		Contents: "# GCS Download Commands for All Detected Files\n# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n",
	}
}

func (m *BucketEnumModule) addFileToLoot(file bucketenumservice.SensitiveFileInfo) {
	// All files go to the general commands file
	m.LootMap["bucket-enum-commands"].Contents += fmt.Sprintf(
		"# [%s] %s - gs://%s/%s\n"+
			"# Category: %s, Size: %d bytes\n"+
			"%s\n\n",
		file.RiskLevel, file.Category,
		file.BucketName, file.ObjectName,
		file.Description, file.Size,
		file.DownloadCmd,
	)

	// CRITICAL and HIGH risk files also go to the sensitive commands file
	if file.RiskLevel == "CRITICAL" || file.RiskLevel == "HIGH" {
		m.LootMap["bucket-enum-sensitive-commands"].Contents += fmt.Sprintf(
			"# [%s] %s - gs://%s/%s\n"+
				"# Category: %s, Size: %d bytes\n"+
				"%s\n\n",
			file.RiskLevel, file.Category,
			file.BucketName, file.ObjectName,
			file.Description, file.Size,
			file.DownloadCmd,
		)
	}
}

func (m *BucketEnumModule) writeOutput(ctx context.Context, logger internal.Logger) {
	// All files table
	header := []string{
		"Project ID",
		"Project Name",
		"Bucket",
		"Object Name",
		"Category",
		"Size",
		"Public",
		"Description",
	}

	var body [][]string
	for _, file := range m.SensitiveFiles {
		publicStatus := "No"
		if file.IsPublic {
			publicStatus = "Yes"
		}

		body = append(body, []string{
			file.ProjectID,
			m.GetProjectName(file.ProjectID),
			file.BucketName,
			file.ObjectName,
			file.Category,
			formatFileSize(file.Size),
			publicStatus,
			file.Description,
		})
	}

	// Critical/High risk files table (sensitive files)
	sensitiveHeader := []string{
		"Project ID",
		"Project Name",
		"Bucket",
		"Object Name",
		"Category",
		"Size",
		"Public",
	}

	var sensitiveBody [][]string
	for _, file := range m.SensitiveFiles {
		if file.RiskLevel == "CRITICAL" || file.RiskLevel == "HIGH" {
			publicStatus := "No"
			if file.IsPublic {
				publicStatus = "Yes"
			}

			sensitiveBody = append(sensitiveBody, []string{
				file.ProjectID,
				m.GetProjectName(file.ProjectID),
				file.BucketName,
				file.ObjectName,
				file.Category,
				formatFileSize(file.Size),
				publicStatus,
			})
		}
	}

	// Collect loot files
	var lootFiles []internal.LootFile
	for _, loot := range m.LootMap {
		if loot.Contents != "" && !strings.HasSuffix(loot.Contents, "# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n") {
			lootFiles = append(lootFiles, *loot)
		}
	}

	tables := []internal.TableFile{
		{
			Name:   "bucket-enum",
			Header: header,
			Body:   body,
		},
	}

	if len(sensitiveBody) > 0 {
		tables = append(tables, internal.TableFile{
			Name:   "bucket-enum-sensitive",
			Header: sensitiveHeader,
			Body:   sensitiveBody,
		})
		logger.InfoM(fmt.Sprintf("[FINDING] Found %d CRITICAL/HIGH risk files!", len(sensitiveBody)), globals.GCP_BUCKETENUM_MODULE_NAME)
	}

	output := BucketEnumOutput{Table: tables, Loot: lootFiles}

	scopeNames := make([]string, len(m.ProjectIDs))
	for i, id := range m.ProjectIDs {
		scopeNames[i] = m.GetProjectName(id)
	}

	err := internal.HandleOutputSmart(
		"gcp",
		m.Format,
		m.OutputDirectory,
		m.Verbosity,
		m.WrapTable,
		"project",
		m.ProjectIDs,
		scopeNames,
		m.Account,
		output,
	)
	if err != nil {
		logger.ErrorM(fmt.Sprintf("Error writing output: %v", err), globals.GCP_BUCKETENUM_MODULE_NAME)
	}
}

func formatFileSize(bytes int64) string {
	const (
		KB = 1024
		MB = KB * 1024
		GB = MB * 1024
	)

	switch {
	case bytes >= GB:
		return fmt.Sprintf("%.1f GB", float64(bytes)/GB)
	case bytes >= MB:
		return fmt.Sprintf("%.1f MB", float64(bytes)/MB)
	case bytes >= KB:
		return fmt.Sprintf("%.1f KB", float64(bytes)/KB)
	default:
		return fmt.Sprintf("%d B", bytes)
	}
}
