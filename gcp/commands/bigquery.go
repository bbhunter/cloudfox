package commands

import (
	"context"
	"fmt"
	"strings"
	"sync"

	BigQueryService "github.com/BishopFox/cloudfox/gcp/services/bigqueryService"
	"github.com/BishopFox/cloudfox/globals"
	"github.com/BishopFox/cloudfox/internal"
	gcpinternal "github.com/BishopFox/cloudfox/internal/gcp"
	"github.com/spf13/cobra"
)

var GCPBigQueryCommand = &cobra.Command{
	Use:     globals.GCP_BIGQUERY_MODULE_NAME,
	Aliases: []string{"bq"},
	Short:   "Enumerate GCP BigQuery datasets and tables with security analysis",
	Long: `Enumerate GCP BigQuery datasets and tables across projects with security-focused analysis.

Features:
- Lists all BigQuery datasets with security-relevant columns
- Shows tables within each dataset with encryption and type info
- Enumerates dataset access control entries (IAM-like)
- Identifies publicly accessible datasets (allUsers/allAuthenticatedUsers)
- Shows encryption status (Google-managed vs CMEK)
- Generates bq commands for data enumeration
- Generates exploitation commands for data access`,
	Run: runGCPBigQueryCommand,
}

// ------------------------------
// Module Struct with embedded BaseGCPModule
// ------------------------------
type BigQueryModule struct {
	gcpinternal.BaseGCPModule

	// Module-specific fields
	Datasets []BigQueryService.BigqueryDataset
	Tables   []BigQueryService.BigqueryTable
	LootMap  map[string]*internal.LootFile
	mu       sync.Mutex
}

// ------------------------------
// Output Struct implementing CloudfoxOutput interface
// ------------------------------
type BigQueryOutput struct {
	Table []internal.TableFile
	Loot  []internal.LootFile
}

func (o BigQueryOutput) TableFiles() []internal.TableFile { return o.Table }
func (o BigQueryOutput) LootFiles() []internal.LootFile   { return o.Loot }

// ------------------------------
// Command Entry Point
// ------------------------------
func runGCPBigQueryCommand(cmd *cobra.Command, args []string) {
	// Initialize command context
	cmdCtx, err := gcpinternal.InitializeCommandContext(cmd, globals.GCP_BIGQUERY_MODULE_NAME)
	if err != nil {
		return // Error already logged
	}

	// Create module instance
	module := &BigQueryModule{
		BaseGCPModule: gcpinternal.NewBaseGCPModule(cmdCtx),
		Datasets:      []BigQueryService.BigqueryDataset{},
		Tables:        []BigQueryService.BigqueryTable{},
		LootMap:       make(map[string]*internal.LootFile),
	}

	// Initialize loot files
	module.initializeLootFiles()

	// Execute enumeration
	module.Execute(cmdCtx.Ctx, cmdCtx.Logger)
}

// ------------------------------
// Module Execution
// ------------------------------
func (m *BigQueryModule) Execute(ctx context.Context, logger internal.Logger) {
	// Run enumeration with concurrency
	m.RunProjectEnumeration(ctx, logger, m.ProjectIDs, globals.GCP_BIGQUERY_MODULE_NAME, m.processProject)

	// Check results
	if len(m.Datasets) == 0 && len(m.Tables) == 0 {
		logger.InfoM("No BigQuery datasets found", globals.GCP_BIGQUERY_MODULE_NAME)
		return
	}

	logger.SuccessM(fmt.Sprintf("Found %d dataset(s) with %d table(s)", len(m.Datasets), len(m.Tables)), globals.GCP_BIGQUERY_MODULE_NAME)

	// Write output
	m.writeOutput(ctx, logger)
}

// ------------------------------
// Project Processor (called concurrently for each project)
// ------------------------------
func (m *BigQueryModule) processProject(ctx context.Context, projectID string, logger internal.Logger) {
	if globals.GCP_VERBOSITY >= globals.GCP_VERBOSE_ERRORS {
		logger.InfoM(fmt.Sprintf("Enumerating BigQuery in project: %s", projectID), globals.GCP_BIGQUERY_MODULE_NAME)
	}

	// Create service and fetch data
	bqService := BigQueryService.New()
	result, err := bqService.BigqueryDatasetsAndTables(projectID)
	if err != nil {
		m.CommandCounter.Error++
		gcpinternal.HandleGCPError(err, logger, globals.GCP_BIGQUERY_MODULE_NAME,
			fmt.Sprintf("Could not enumerate BigQuery in project %s", projectID))
		return
	}

	// Thread-safe append
	m.mu.Lock()
	m.Datasets = append(m.Datasets, result.Datasets...)
	m.Tables = append(m.Tables, result.Tables...)

	// Generate loot for each dataset and table
	for _, dataset := range result.Datasets {
		m.addDatasetToLoot(dataset)
	}
	for _, table := range result.Tables {
		m.addTableToLoot(table)
	}
	m.mu.Unlock()

	if globals.GCP_VERBOSITY >= globals.GCP_VERBOSE_ERRORS {
		logger.InfoM(fmt.Sprintf("Found %d dataset(s) and %d table(s) in project %s", len(result.Datasets), len(result.Tables), projectID), globals.GCP_BIGQUERY_MODULE_NAME)
	}
}

// ------------------------------
// Loot File Management
// ------------------------------
func (m *BigQueryModule) initializeLootFiles() {
	m.LootMap["bigquery-commands"] = &internal.LootFile{
		Name:     "bigquery-commands",
		Contents: "# GCP BigQuery Commands\n# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n",
	}
}

func (m *BigQueryModule) addDatasetToLoot(dataset BigQueryService.BigqueryDataset) {
	// All commands for this dataset
	m.LootMap["bigquery-commands"].Contents += fmt.Sprintf(
		"## Dataset: %s (Project: %s, Location: %s)\n"+
			"# Show dataset info\n"+
			"bq show --project_id=%s %s\n"+
			"bq show --format=prettyjson %s:%s\n\n"+
			"# List tables in dataset\n"+
			"bq ls --project_id=%s %s\n\n",
		dataset.DatasetID, dataset.ProjectID, dataset.Location,
		dataset.ProjectID, dataset.DatasetID,
		dataset.ProjectID, dataset.DatasetID,
		dataset.ProjectID, dataset.DatasetID,
	)
}

func (m *BigQueryModule) addTableToLoot(table BigQueryService.BigqueryTable) {
	// Table info and query commands
	m.LootMap["bigquery-commands"].Contents += fmt.Sprintf(
		"## Table: %s.%s (Project: %s)\n"+
			"# Type: %s, Size: %d bytes, Rows: %d\n"+
			"# Show table schema:\n"+
			"bq show --schema --project_id=%s %s:%s.%s\n"+
			"# Query first 100 rows:\n"+
			"bq query --project_id=%s --use_legacy_sql=false 'SELECT * FROM `%s.%s.%s` LIMIT 100'\n"+
			"# Export table to GCS:\n"+
			"bq extract --project_id=%s '%s:%s.%s' gs://<bucket>/export_%s_%s.json\n\n",
		table.DatasetID, table.TableID, table.ProjectID,
		table.TableType, table.NumBytes, table.NumRows,
		table.ProjectID, table.ProjectID, table.DatasetID, table.TableID,
		table.ProjectID, table.ProjectID, table.DatasetID, table.TableID,
		table.ProjectID, table.ProjectID, table.DatasetID, table.TableID, table.DatasetID, table.TableID,
	)

	// Views (may expose data from other datasets)
	if table.IsView {
		viewQuery := table.ViewQuery
		if len(viewQuery) > 200 {
			viewQuery = viewQuery[:200] + "..."
		}
		m.LootMap["bigquery-commands"].Contents += fmt.Sprintf(
			"# VIEW DEFINITION: %s.%s\n"+
				"# Legacy SQL: %v\n"+
				"# Query:\n"+
				"# %s\n\n",
			table.DatasetID, table.TableID,
			table.UseLegacySQL,
			strings.ReplaceAll(viewQuery, "\n", "\n# "),
		)
	}
}

// ------------------------------
// Output Generation
// ------------------------------
func (m *BigQueryModule) writeOutput(ctx context.Context, logger internal.Logger) {
	// Dataset table with access columns (one row per access entry)
	datasetHeader := []string{
		"Project ID",
		"Project Name",
		"Dataset ID",
		"Location",
		"Public",
		"Encryption",
		"Role",
		"Member Type",
		"Member",
	}

	var datasetBody [][]string
	publicCount := 0
	for _, dataset := range m.Datasets {
		publicStatus := ""
		if dataset.IsPublic {
			publicStatus = dataset.PublicAccess
			publicCount++
		}

		// One row per access entry
		if len(dataset.AccessEntries) > 0 {
			for _, entry := range dataset.AccessEntries {
				memberType := BigQueryService.GetMemberType(entry.EntityType, entry.Entity)
				role := entry.Role
				// Special access types (View, Routine, Dataset) may not have explicit roles
				if role == "" {
					role = "READER" // Views/Routines/Datasets grant implicit read access
				}
				datasetBody = append(datasetBody, []string{
					dataset.ProjectID,
					m.GetProjectName(dataset.ProjectID),
					dataset.DatasetID,
					dataset.Location,
					publicStatus,
					dataset.EncryptionType,
					role,
					memberType,
					entry.Entity,
				})
			}
		} else {
			// Dataset with no access entries
			datasetBody = append(datasetBody, []string{
				dataset.ProjectID,
				m.GetProjectName(dataset.ProjectID),
				dataset.DatasetID,
				dataset.Location,
				publicStatus,
				dataset.EncryptionType,
				"-",
				"-",
				"-",
			})
		}
	}

	// Table table with security columns (one row per IAM binding member)
	tableHeader := []string{
		"Project ID",
		"Project Name",
		"Dataset ID",
		"Table ID",
		"Type",
		"Encryption",
		"Rows",
		"Public",
		"Role",
		"Member",
	}

	var tableBody [][]string
	publicTableCount := 0
	for _, table := range m.Tables {
		publicStatus := ""
		if table.IsPublic {
			publicStatus = table.PublicAccess
			publicTableCount++
		}

		// If no IAM bindings, still show the table
		if len(table.IAMBindings) == 0 {
			tableBody = append(tableBody, []string{
				table.ProjectID,
				m.GetProjectName(table.ProjectID),
				table.DatasetID,
				table.TableID,
				table.TableType,
				table.EncryptionType,
				fmt.Sprintf("%d", table.NumRows),
				publicStatus,
				"-",
				"-",
			})
		} else {
			// One row per member per role
			for _, binding := range table.IAMBindings {
				for _, member := range binding.Members {
					tableBody = append(tableBody, []string{
						table.ProjectID,
						m.GetProjectName(table.ProjectID),
						table.DatasetID,
						table.TableID,
						table.TableType,
						table.EncryptionType,
						fmt.Sprintf("%d", table.NumRows),
						publicStatus,
						binding.Role,
						member,
					})
				}
			}
		}
	}

	// Collect loot files
	var lootFiles []internal.LootFile
	for _, loot := range m.LootMap {
		if loot.Contents != "" && !strings.HasSuffix(loot.Contents, "# Generated by CloudFox\n# WARNING: Only use with proper authorization\n\n") {
			lootFiles = append(lootFiles, *loot)
		}
	}

	// Build tables list
	tables := []internal.TableFile{
		{
			Name:   "bigquery-datasets",
			Header: datasetHeader,
			Body:   datasetBody,
		},
		{
			Name:   "bigquery-tables",
			Header: tableHeader,
			Body:   tableBody,
		},
	}

	if publicCount > 0 {
		logger.InfoM(fmt.Sprintf("[FINDING] Found %d publicly accessible dataset(s)!", publicCount), globals.GCP_BIGQUERY_MODULE_NAME)
	}

	output := BigQueryOutput{
		Table: tables,
		Loot:  lootFiles,
	}

	// Write output using HandleOutputSmart with scope support
	scopeNames := make([]string, len(m.ProjectIDs))
	for i, id := range m.ProjectIDs {
		scopeNames[i] = m.GetProjectName(id)
	}
	err := internal.HandleOutputSmart(
		"gcp",
		m.Format,
		m.OutputDirectory,
		m.Verbosity,
		m.WrapTable,
		"project",    // scopeType
		m.ProjectIDs, // scopeIdentifiers
		scopeNames,   // scopeNames
		m.Account,
		output,
	)
	if err != nil {
		logger.ErrorM(fmt.Sprintf("Error writing output: %v", err), globals.GCP_BIGQUERY_MODULE_NAME)
		m.CommandCounter.Error++
	}
}
